{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with `LangChain` & `OpenAI` for Document QA within long documents\n",
    "\n",
    "Leveraging the LangChain framework to build Document QA Tools that use ChatGPT to extract information and present it in humanly form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import credentials\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = credentials.openai_api\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, TokenTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. QA (without `retriever` object)\n",
    "\n",
    "With retriever object : https://python.langchain.com/en/latest/modules/chains/index_examples/vector_db_qa.html\n",
    "\n",
    "https://python.langchain.com/en/latest/use_cases/question_answering.html\n",
    "https://python.langchain.com/en/latest/modules/chains/index_examples/question_answering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"../docs/chatgpt_info_extraction.pdf\")\n",
    "docs = loader.load_and_split(text_splitter = TokenTextSplitter(encoding_name = 'cl100k_base'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate how many tokens will be just in the document itself (no prompt, no output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22284\n"
     ]
    }
   ],
   "source": [
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "whole_doc = ' '.join([i.page_content for i in docs])\n",
    "tokens_in_doc = encoding.encode(whole_doc)\n",
    "\n",
    "print(len(tokens_in_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store document parts in vectorDB, so only relevant parts get passed to LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(docs, OpenAIEmbeddings(model = 'gpt-3.5-turbo'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create chain but wrap in function that pre-selects relevant parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_chain(ChatOpenAI(temperature=0.0), chain_type=\"stuff\")\n",
    "\n",
    "def run_query(query, max_num_rel_docs = 2):\n",
    "    relevant_docs = db.similarity_search(query, k = max_num_rel_docs)\n",
    "    answer = chain.run(input_documents = relevant_docs, question = query)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\"Who are the authors?\",\n",
    "           \"What was the main experiment?\",\n",
    "           \"What were the main outcomes?\",\n",
    "           \"Other than ChatGPT, which other language models did the researchers experiment with?\",\n",
    "           \"What is the comparison between models in terms of performance?\",\n",
    "           \"What are the most important takeaways?\",\n",
    "           \"How many papers are referenced?\",\n",
    "           \"What are some links or URLs present in the paper?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who are the authors?\n",
      "Answer:\n",
      "There are multiple authors listed for different papers in this context. Please specify which paper you are referring to.\n",
      "\n",
      " ----------------------------------------------------------------------\n",
      "Question: What was the main experiment?\n",
      "Answer:\n",
      "The main experiment was to evaluate the performance of ChatGPT, a language model, on a diverse range of Information Extraction (IE) tasks, including 7 fine-grained tasks spanning 14 datasets. The study collected 15 keys for each dataset from both ChatGPT and domain experts and compared ChatGPT's performance with several popular baselines. The aim was to analyze ChatGPT's abilities without any training and to understand its performance on different IE tasks. The study also compared ChatGPT's performance on both Standard-IE and OpenIE settings.\n",
      "\n",
      " ----------------------------------------------------------------------\n",
      "Question: What were the main outcomes?\n",
      "Answer:\n",
      "The text provides several outcomes of the research on ChatGPT's performance in Information Extraction (IE) tasks. \n",
      "\n",
      "Firstly, the table in section A.3 shows the performance of ChatGPT and several baseline models on 14 IE datasets on the Standard-IE setting. The results indicate that ChatGPT outperforms or performs comparably to the baseline models on most tasks.\n",
      "\n",
      "Secondly, the results in Table 3 show that ChatGPT's performance on the OpenIE setting is somewhat inspiring, with more than 84% of the predictions considered reasonable by domain experts in ET, NER, and RC tasks. However, the performance is relatively poorer for more challenging tasks, such as RE and EE.\n",
      "\n",
      "Thirdly, the top-k recall analysis in section 4.4 shows that ChatGPT is a competent answer candidate generator for a given task under the Standard-IE setting, which could help users select the most probable prediction from the top-5 predictions.\n",
      "\n",
      "Finally, the text mentions the importance of evaluating ChatGPT's explainability, calibration, and faithfulness, but does not provide specific outcomes for these evaluations.\n",
      "\n",
      " ----------------------------------------------------------------------\n",
      "Question: Other than ChatGPT, which other language models did the researchers experiment with?\n",
      "Answer:\n",
      "The article does not mention any other language models that the researchers experimented with. The focus of the article is on evaluating the overall ability of ChatGPT using various information extraction tasks and measuring its performance, explainability, calibration, and faithfulness.\n",
      "\n",
      " ----------------------------------------------------------------------\n",
      "Question: What is the comparison between models in terms of performance?\n",
      "Answer:\n",
      "According to the research, ChatGPT's performance is not satisfactory in the Standard-IE setting, but it surprisingly exhibits excellent performance in the OpenIE setting, as confirmed by human evaluation. The study also compares ChatGPT's performance with other popular models, but it does not provide specific details on the comparison results.\n",
      "\n",
      " ----------------------------------------------------------------------\n",
      "Question: What are the most important takeaways?\n",
      "Answer:\n",
      "I'm sorry, but I cannot provide a summary of the most important takeaways without additional context. The given text includes information about various research papers and datasets related to computational linguistics and natural language processing. If you could provide more specific information or context, I would be happy to help you with your question.\n",
      "\n",
      " ----------------------------------------------------------------------\n",
      "Question: How many papers are referenced?\n",
      "Answer:\n",
      "There are 28 papers referenced.\n",
      "\n",
      " ----------------------------------------------------------------------\n",
      "Question: What are some links or URLs present in the paper?\n",
      "Answer:\n",
      "I'm sorry, but I cannot provide a list of links or URLs present in the paper as the context provided does not contain any such information.\n",
      "\n",
      " ----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for query in queries:\n",
    "    print('Question:', query)\n",
    "    print('Answer:')\n",
    "    print(run_query(query))\n",
    "    print('\\n', '-------' * 10)\n",
    "    time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of these anwers are satisfactory, but due to using limited docs for input, ChatGPT does not see the whole context. With a paid plan there is no 3/min rate limitation, then I can pass the whole document into the prompt (with map reduce chaining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Summarization\n",
    "\n",
    "https://python.langchain.com/en/latest/modules/chains/index_examples/summarize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(ChatOpenAI(temperature=0.0), chain_type=\"map_reduce\")\n",
    "summary = chain.run(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The articles and papers listed cover various topics related to natural language processing, including the evaluation of ChatGPT'\n",
      "s performance in information extraction tasks, the use of language models for entity and relation extraction, and the potential \n",
      "impact of AI on online exam integrity. The studies explore the capabilities and limitations of language models, as well as their\n",
      " potential applications in fields such as medicine and translation. The articles also discuss techniques for improving the accur\n",
      "acy and efficiency of natural language processing models.\n"
     ]
    }
   ],
   "source": [
    "print(re.sub(\"(.{128})\", \"\\\\1\\n\", summary, 0, re.DOTALL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Write a concise summary of the following:\n",
    "\n",
    "{text}\n",
    "\n",
    "CONCISE SUMMARY IN HUNGARIAN:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "chain = load_summarize_chain(ChatOpenAI(temperature=0.0), chain_type=\"map_reduce\", map_prompt=PROMPT, combine_prompt=PROMPT)\n",
    "summary = chain.run(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A cikkgyűjtemény bemutatja a mesterséges intelligencia és a természetes nyelv feldolgozásának fejlesztésére és javítására összpo\n",
      "ntosító kutatásokat. Az ACL 2020 és az ACL 2022 konferenciákon bemutatott kutatások között szerepelnek olyan témák, mint az absz\n",
      "trakt összefoglalások hűséges és valósághűségi kérdései, a chatbotok döntéshozatalának magyarázata, a nyelvi homályosság elemzés\n",
      "e, a nagy nyelvi modellek külső tudás és automatikus visszajelzés felhasználásával történő javítása, valamint a nyelvi modellek \n",
      "által tükrözött vélemények kérdése. Az A mellékletben bemutatják az egyes feladatokhoz használt adathalmazokat, valamint a legúj\n",
      "abb módszereket is bemutatják minden adathalmazon, amelyek a legjobb eredményeket érték el az adott feladatokban.\n"
     ]
    }
   ],
   "source": [
    "print(re.sub(\"(.{128})\", \"\\\\1\\n\", summary, 0, re.DOTALL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check API usage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.api_requestor import APIRequestor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = APIRequestor();\n",
    "resp = r.request(\"GET\", '/usage?date=2023-04-25'); \n",
    "resp_object = resp[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['object', 'data', 'ft_data', 'dalle_api_data', 'whisper_api_data', 'current_usage_usd'])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp_object.data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openAI_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
