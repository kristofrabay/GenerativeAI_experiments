{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Familiarization with LLM packages such as `LangChain`, `OpenAI` & `tiktoken`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import credentials"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tiktoken\n",
    "\n",
    "https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "\n",
    "\n",
    "Encodings specify how text is converted into tokens. Different models use different encodings\n",
    "\n",
    "`tiktoken` supports three encodings used by OpenAI models:\n",
    "\n",
    "| Encoding name           | OpenAI models                                       |\n",
    "|-------------------------|-----------------------------------------------------|\n",
    "| `cl100k_base`           | `gpt-4`, `gpt-3.5-turbo`, `text-embedding-ada-002`  |\n",
    "| `p50k_base`             | Codex models, `text-davinci-002`, `text-davinci-003`|\n",
    "| `r50k_base` (or `gpt2`) | GPT-3 models like `davinci`                         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "#encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[91273, 433, 690, 11422, 304, 70695, 13]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Tomorrow it will rain in Budapest.'\n",
    "encoding.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[91273, 275, 14724, 30193, 258, 33, 664, 28724, 13]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'TomorrowitwillraininBudapest.'\n",
    "encoding.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'Tomorrow', b'it', b'will', b'rain', b'in', b'B', b'ud', b'apest', b'.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[encoding.decode_single_token_bytes(token) for token in [91273, 275, 14724, 30193, 258, 33, 664, 28724, 13]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tomorrow', 'it', 'will', 'rain', 'in', 'B', 'ud', 'apest', '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[bytes.decode(encoding.decode_single_token_bytes(token)) for token in [91273, 275, 14724, 30193, 258, 33, 664, 28724, 13]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' it'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.decode([433])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.decode([275])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. `LangChain`, `Pinecone` & other vector databases / vector representers for Document Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"../docs/investment_report.pdf\")\n",
    "#doc = loader.load_and_split()\n",
    "doc = loader.load_and_split(text_splitter = RecursiveCharacterTextSplitter(chunk_size = 4000, chunk_overlap = 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2991\n",
      "3986\n",
      "549\n",
      "3977\n",
      "632\n",
      "3869\n",
      "2733\n"
     ]
    }
   ],
   "source": [
    "for i in doc:\n",
    "    print(len(i.page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quarterly Investment Report \n",
      "April 2023\n",
      "Despite banking instability and uncertain monetary policy expectations, the U.S. economy ended the quarter on  \n",
      "a positive note. The S&P 500 Index rose 3.7% in March and 7.5% in the first quarter. The U.S. markets outperformed \n",
      "most major international markets. During the quarter, bond prices moved up, pushing Treasury yields in 1-year  and longer maturities\n"
     ]
    }
   ],
   "source": [
    "print(doc[0].page_content[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': '../docs/investment_report.pdf', 'page': 0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 pages in this PDF\n"
     ]
    }
   ],
   "source": [
    "page_num = doc[-1].metadata['page']\n",
    "print('There are', page_num, 'pages in this PDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_embedder = OpenAIEmbeddings(openai_api_key = credentials.openai_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_embed_result = openai_embedder.embed_query('Embed this text!')\n",
    "len(openai_embed_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1536\n"
     ]
    }
   ],
   "source": [
    "openai_embed_results = openai_embedder.embed_documents(['Embed this text!', 'This one too, please.'])\n",
    "print(len(openai_embed_results))\n",
    "print(len(openai_embed_results[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rabay_Kristof\\Documents\\Python Scripts\\openAI_experiments\\openAI_venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "hf_embedder = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_embed_result = hf_embedder.embed_query('Embed this text!')\n",
    "len(hf_embed_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "hf_embed_results = hf_embedder.embed_documents(['Embed this text!', 'This one too, please.'])\n",
    "print(len(hf_embed_results))\n",
    "print(len(hf_embed_results[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector databases\n",
    "\n",
    "https://python.langchain.com/en/latest/modules/indexes/vectorstores.html?highlight=vectorstores\n",
    "\n",
    "For some, like `Pinecone`, we need an API connection.\n",
    "For others we can simply store them locally, e.g. in a pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(doc, hf_embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "query = 'What happened in the bond market during the past quarter?'\n",
    "relevant_docs = db.similarity_search(query)\n",
    "\n",
    "print(len(relevant_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  ---  0.72051513\n",
      "1  ---  0.87298733\n",
      "2  ---  1.0739416\n",
      "3  ---  1.0902194\n"
     ]
    }
   ],
   "source": [
    "for e, i in enumerate(db.similarity_search_with_score(query)):\n",
    "    print(e, ' --- ', i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  ---  0.4905188642503674\n",
      "1  ---  0.38270473909543834\n",
      "2  ---  0.24060862024283913\n",
      "3  ---  0.22909848450190795\n"
     ]
    }
   ],
   "source": [
    "for e, i in enumerate(db.similarity_search_with_relevance_scores(query)):\n",
    "    print(e, ' --- ', i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. `Pinecone`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "pinecone.init(api_key = credentials.pinecone_api,\n",
    "              environment = credentials.pinecone_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"first-experiment-sbert\"\n",
    "index = Pinecone.from_documents(doc, hf_embedder, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating QA chain without OpenAI (will be using `flan-t5-large`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceHub(repo_id = \"google/flan-t5-large\", \n",
    "                     model_kwargs={\"temperature\":0, \"max_length\":512},\n",
    "                     huggingfacehub_api_token = credentials.huggingface_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_chain(llm, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The yield curve remains inverted'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What will happen to the bond market's yield curve?\"\n",
    "docs = db.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The yield curve remains inverted indicating investors’ expectations that while the Fed will continue to exert upward pressure on short rates, longer maturity rates remain relatively low, reflecting the likelihood of economic slowdown and even possibly a recession.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Will the bond market yield curve change?\"\n",
    "docs = db.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The unemployment rate increased to 3.6% and the number of unemployed people increased to 5.9 million in February.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What's happening to unemployment?\"\n",
    "docs = db.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Silicon Valley Bank and Signature Bank'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Which banks experienced stress? Did any of them default?\"\n",
    "docs = db.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The failure of two big U.S. banks'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Why did Silicon Valley Bank default?\"\n",
    "docs = db.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'weak underwriting standards and excessive leverage'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Why did Silicon Valley Bank fail?\"\n",
    "docs = db.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"When was Messi born?\"\n",
    "docs = db.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrapper around loaders, vectorstores, embeddings, text-splitters: `VectorstoreIndexCreator`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_retriever = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=FAISS,\n",
    "    embedding = OpenAIEmbeddings(openai_api_key=credentials.openai_api),\n",
    "    text_splitter=RecursiveCharacterTextSplitter()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"../docs/investment_report.pdf\")\n",
    "index = index_retriever.from_loaders([loader])\n",
    "db_retriever = index.vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm = llm, chain_type=\"stuff\", retriever = db_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The unemployment rate increased to 3.6% and the number of unemployed people increased to 5.9 million in February.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What's happening to unemployment?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Silicon Valley Bank (SVB) and Signature Bank'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Which banks experienced stress? Did any of them default?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`flan-t5` occasionally hallucinates, extracts info from out-of-context. This can be avoided by using `OpenAI`'s `GPT` models. Additionally, we can try and guide these models with `PromptTemplates`. Let's see if we can make `flan-t5` a little bit smarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rachael'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the first name of the current Singapore President?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the name of the current Singapore President?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'not enough information'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How do you make scrambled eggs?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Space'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How do you go to the Moon?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"If you don't know the answer, just say that you don't know, don't try to make up an answer.\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the Danube?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PromptTemplate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_custom_prompt = RetrievalQA.from_chain_type(llm = llm, chain_type=\"stuff\", retriever = db_retriever, chain_type_kwargs=chain_type_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Silicon Valley Bank (SVB) and Signature Bank'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Which banks experienced stress? Did any of them default?\"\n",
    "qa_custom_prompt.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rafik'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the first name of the current Singapore President?\"\n",
    "qa_custom_prompt.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Space'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How do you go to the Moon?\"\n",
    "qa_custom_prompt.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'European'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the Danube?\"\n",
    "qa_custom_prompt.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a bowl of water'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How do you make scrambled eggs?\"\n",
    "qa_custom_prompt.run(query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different, but still bad. Need to use `OpenAI` models + prompts to avoid this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. `OpenAI`\n",
    "\n",
    "https://github.com/hwchase17/langchain/blob/master/docs/getting_started/getting_started.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import AIMessage, ChatMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InstrucGPT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nQ: What did the fish say when it hit the wall?\\nA: Dam!'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('InstrucGPT')\n",
    "llm = OpenAI(model_name='text-davinci-003', openai_api_key=credentials.openai_api)\n",
    "llm('Tell me a joke')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGPT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the tomato turn red? Because it saw the salad dressing!', additional_kwargs={})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('ChatGPT')\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', openai_api_key=credentials.openai_api)\n",
    "llm(([HumanMessage(content = \"Tell me a joke\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens('Tell me a joke.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens('Tellmeajoke.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add `SystemMessage` to prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temperature set to 0 to be as precise as possible\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', openai_api_key=credentials.openai_api, temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [\n",
    "    SystemMessage(content=\"You are a helpful assistant to a comedian. You come up with funny material on as many topics as possible.\"),\n",
    "    HumanMessage(content=\"Come up with a joke about a giraffe and a lion.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the giraffe invite the lion to his party? Because he wanted to have a \"tall\" order of protection!', additional_kwargs={})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate outputs in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_messages = [\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to Russian.\"),\n",
    "        HumanMessage(content=\"Translate this sentence from English to Russian: I love programming.\")\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that comes up with catchy company names.\"),\n",
    "        HumanMessage(content=\"Create a name for a company that mass produces aluminium foil.\")\n",
    "    ],\n",
    "]\n",
    "result = llm.generate(batch_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'prompt_tokens': 74,\n",
       "  'completion_tokens': 11,\n",
       "  'total_tokens': 85},\n",
       " 'model_name': 'gpt-3.5-turbo'}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[ChatGeneration(text='Я люблю программирование.', generation_info=None, message=AIMessage(content='Я люблю программирование.', additional_kwargs={}))],\n",
       " [ChatGeneration(text='FoilWorks', generation_info=None, message=AIMessage(content='FoilWorks', additional_kwargs={}))]]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Я люблю программирование.\n",
      "FoilWorks\n"
     ]
    }
   ],
   "source": [
    "for i in result.generations:\n",
    "    print(i[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add `Memory` to ChatBot so you can refer back to previous topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "conversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input = 'Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Hundred Year War was a series of conflicts between England and France that lasted from 1337 to 1453. The main cause of the war was a dispute over the succession to the French throne, as the English king claimed to be the rightful heir. Other factors included economic and territorial disputes, as well as long-standing tensions between the two nations.'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input = 'Can you briefly tell me the reason behind the Hunder Year War? Please keep it short and concise.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, there were several major conflicts between England and France throughout history. Some of the most notable ones include the Norman Conquest of England in 1066, the Battle of Hastings in 1066, the Battle of Agincourt in 1415, and the Battle of Waterloo in 1815. These conflicts were often driven by political, economic, and territorial disputes, as well as cultural and religious differences between the two nations.'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input = 'Where there any other major conflicts between these two countries?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT remembers (with memory) that the 2 countries here are England and France"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hello', additional_kwargs={}),\n",
       " AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}),\n",
       " HumanMessage(content='Can you briefly tell me the reason behind the Hunder Year War? Please keep it short and concise.', additional_kwargs={}),\n",
       " AIMessage(content='The Hundred Year War was a series of conflicts between England and France that lasted from 1337 to 1453. The main cause of the war was a dispute over the succession to the French throne, as the English king claimed to be the rightful heir. Other factors included economic and territorial disputes, as well as long-standing tensions between the two nations.', additional_kwargs={}),\n",
       " HumanMessage(content='Where there any other major conflicts between these two countries?', additional_kwargs={}),\n",
       " AIMessage(content='Yes, there were several major conflicts between England and France throughout history. Some of the most notable ones include the Norman Conquest of England in 1066, the Battle of Hastings in 1066, the Battle of Agincourt in 1415, and the Battle of Waterloo in 1815. These conflicts were often driven by political, economic, and territorial disputes, as well as cultural and religious differences between the two nations.', additional_kwargs={})]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.memory.chat_memory.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. `Langchain` agents - most powerful tool within framework\n",
    "\n",
    "Agents use an LLM to determine which actions to take and in what order.\n",
    "\n",
    "Tools: https://python.langchain.com/en/latest/modules/agents/tools.html <br>\n",
    "Agents: https://python.langchain.com/en/latest/modules/agents/agents/agent_types.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say I want to find some information on the internet, a number, then use that number in a mathematical expression. I'll need a tool to search the web, then I'll need a tool to solve math problems"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: What's the 3rd base log of the number that is the year when Messi won his first Ballon d'Or? --> X = log3(YEAR) \n",
    "\n",
    "We'll try to find the info in `Wikipedia`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature = 0.0, openai_api_key = credentials.openai_api)\n",
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-3.5-turbo'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to find out the year when Messi won his first Ballon d'Or and then calculate the 3rd base log of that number.\n",
      "Action: Wikipedia\n",
      "Action Input: \"Lionel Messi Ballon d'Or\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: 2019 Ballon d'Or\n",
      "Summary: The 2019 Ballon d'Or  (French: Ballon d'Or) was the 64th annual ceremony of the Ballon d'Or, presented by France Football, and recognising the best footballers in the world for 2019. Lionel Messi won the men's award for a record sixth time in his career.\n",
      "\n",
      "Page: 2009 Ballon d'Or\n",
      "Summary: The 2009 Ballon d'Or, given to the best football player in the world as judged by an international panel of sports journalists, was awarded to Lionel Messi of Barcelona on 1 December 2009.Messi won the award by a then record margin, 240 points ahead of 2008 winner Cristiano Ronaldo. Xavi was the second Barcelona player in the top three, finishing a further 63 points behind Ronaldo. Messi's win made him the first Argentine-born player to win the award since Omar Sívori in 1961; however, Sívori had taken Italian citizenship by that time and is recognised to have won the Ballon d'Or as an Italian player.\n",
      "\n",
      "Page: FIFA Ballon d'Or\n",
      "Summary: The FIFA Ballon d'Or (\"Golden Ball\") was an annual association football award presented to the world's best men's player from 2010 to 2015. Awarded jointly by FIFA and France Football, the prize was a merger of the FIFA World Player of the Year award and the Ballon d'Or, the two most prestigious individual honours in world football. Unlike the Ballon d'Or awarded by France Football, the FIFA Ballon d'Or, was not awarded based on votes from international journalists, but votes from national team coaches and captains, who selected the players they deemed to have performed the best in the previous calendar year.\n",
      "The six editions of the FIFA Ballon d'Or were dominated by Lionel Messi and Cristiano Ronaldo, as part of their ongoing rivalry.\n",
      "Messi, who played for Barcelona, won the inaugural Ballon d'Or in 2010 and went on to win three in a row, after his wins in 2011 and 2012. Ronaldo, who played for Real Madrid, won successive awards in the next two years. The final FIFA Ballon d'Or was presented to Messi in 2015. Its awarding bodies subsequently ended their partnership; for 2016, France Football reintroduced the previous format of the Ballon d'Or, while FIFA created The Best FIFA Men's Player award.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to search for the year when Messi won his first Ballon d'Or.\n",
      "Action: Wikipedia\n",
      "Action Input: \"Lionel Messi Ballon d'Or 2009\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: 2009 Ballon d'Or\n",
      "Summary: The 2009 Ballon d'Or, given to the best football player in the world as judged by an international panel of sports journalists, was awarded to Lionel Messi of Barcelona on 1 December 2009.Messi won the award by a then record margin, 240 points ahead of 2008 winner Cristiano Ronaldo. Xavi was the second Barcelona player in the top three, finishing a further 63 points behind Ronaldo. Messi's win made him the first Argentine-born player to win the award since Omar Sívori in 1961; however, Sívori had taken Italian citizenship by that time and is recognised to have won the Ballon d'Or as an Italian player.\n",
      "\n",
      "Page: 2008 Ballon d'Or\n",
      "Summary: The 2008 Ballon d'Or, given to the best football player in the world as judged by an international panel of sports journalists, was awarded on 2 December 2008. Cristiano Ronaldo won the award, being his first of five. His win also began a 10-year dominance of the award between him and Lionel Messi  — which eventually led to an intense rivalry between the two. Both players had won five Ballon d'Or awards before the dominance was ended in 2018.\n",
      "\n",
      "\n",
      "\n",
      "Page: FIFA Ballon d'Or\n",
      "Summary: The FIFA Ballon d'Or (\"Golden Ball\") was an annual association football award presented to the world's best men's player from 2010 to 2015. Awarded jointly by FIFA and France Football, the prize was a merger of the FIFA World Player of the Year award and the Ballon d'Or, the two most prestigious individual honours in world football. Unlike the Ballon d'Or awarded by France Football, the FIFA Ballon d'Or, was not awarded based on votes from international journalists, but votes from national team coaches and captains, who selected the players they deemed to have performed the best in the previous calendar year.\n",
      "The six editions of the FIFA Ballon d'Or were dominated by Lionel Messi and Cristiano Ronaldo, as part of their ongoing rivalry.\n",
      "Messi, who played for Barcelona, won the inaugural Ballon d'Or in 2010 and went on to win three in a row, after his wins in 2011 and 2012. Ronaldo, who played for Real Madrid, won successive awards in the next two years. The final FIFA Ballon d'Or was presented to Messi in 2015. Its awarding bodies subsequently ended their partnership; for 2016, France Football reintroduced the previous format of the Ballon d'Or, while FIFA created The Best FIFA Men's Player award.\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-MlWDteCoYCC2gwjf4Z5jxjJh on requests per min. Limit: 3 / min. Please try again in 20s. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mI have found that Messi won his first Ballon d'Or in 2009, now I need to calculate the 3rd base log of that number.\n",
      "Action: Calculator\n",
      "Action Input: log(2009)/log(3)\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-MlWDteCoYCC2gwjf4Z5jxjJh on requests per min. Limit: 3 / min. Please try again in 20s. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-MlWDteCoYCC2gwjf4Z5jxjJh on requests per min. Limit: 3 / min. Please try again in 20s. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-MlWDteCoYCC2gwjf4Z5jxjJh on requests per min. Limit: 3 / min. Please try again in 20s. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 6.9227264643428\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-MlWDteCoYCC2gwjf4Z5jxjJh on requests per min. Limit: 3 / min. Please try again in 20s. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-MlWDteCoYCC2gwjf4Z5jxjJh on requests per min. Limit: 3 / min. Please try again in 20s. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-MlWDteCoYCC2gwjf4Z5jxjJh on requests per min. Limit: 3 / min. Please try again in 20s. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 16.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-MlWDteCoYCC2gwjf4Z5jxjJh on requests per min. Limit: 3 / min. Please try again in 20s. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
      "Final Answer: 6.9227264643428\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'6.9227264643428'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem = \"What's the 3rd base log of the number that is the year when Messi won his first Ballon d'Or?\"\n",
    "agent.run(problem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is correct, Messi won his first Ballon d'Or in 2009. Log(3)(2009) is 6.922."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openAI_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
